{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce RTX 3090\n",
      "Memory Usage:\n",
      "Allocated: 0.1 GB\n",
      "Cached:    4.1 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.models as models\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to a consistent size\n",
    "    transforms.ToTensor(),          # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize image data\n",
    "])\n",
    "\n",
    "# Set the paths for the datasets\n",
    "base_folder = \"../datasets/Vision_data\"\n",
    "train_folder = os.path.join(base_folder, \"train\")\n",
    "test_folder = os.path.join(base_folder, \"test\")\n",
    "validation_folder = os.path.join(base_folder, \"validation\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ImageFolder(root=train_folder, transform=transform)\n",
    "test_dataset = ImageFolder(root=test_folder, transform=transform)\n",
    "validation_dataset = ImageFolder(root=validation_folder, transform=transform)\n",
    "\n",
    "batchsize = 64\n",
    "numworkers = 4\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batchsize, shuffle=True, num_workers=numworkers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batchsize, shuffle=False, num_workers=numworkers)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batchsize, shuffle=False, num_workers=numworkers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 379/379 [06:34<00:00,  1.04s/it]\n",
      "Epoch 2/20: 100%|██████████| 379/379 [06:35<00:00,  1.04s/it]\n",
      "Epoch 3/20: 100%|██████████| 379/379 [06:34<00:00,  1.04s/it]\n",
      "Epoch 4/20: 100%|██████████| 379/379 [06:31<00:00,  1.03s/it]\n",
      "Epoch 5/20: 100%|██████████| 379/379 [06:34<00:00,  1.04s/it]\n",
      "Epoch 6/20: 100%|██████████| 379/379 [06:36<00:00,  1.05s/it]\n",
      "Epoch 7/20: 100%|██████████| 379/379 [06:19<00:00,  1.00s/it]\n",
      "Epoch 8/20: 100%|██████████| 379/379 [06:26<00:00,  1.02s/it]\n",
      "Epoch 9/20: 100%|██████████| 379/379 [06:33<00:00,  1.04s/it]\n",
      "Epoch 10/20: 100%|██████████| 379/379 [06:34<00:00,  1.04s/it]\n",
      "Epoch 11/20: 100%|██████████| 379/379 [06:35<00:00,  1.04s/it]\n",
      "Epoch 12/20: 100%|██████████| 379/379 [06:32<00:00,  1.04s/it]\n",
      "Epoch 13/20: 100%|██████████| 379/379 [06:39<00:00,  1.05s/it]\n",
      "Epoch 14/20: 100%|██████████| 379/379 [06:29<00:00,  1.03s/it]\n",
      "Epoch 15/20: 100%|██████████| 379/379 [06:28<00:00,  1.02s/it]\n",
      "Epoch 16/20: 100%|██████████| 379/379 [06:22<00:00,  1.01s/it]\n",
      "Epoch 17/20: 100%|██████████| 379/379 [06:25<00:00,  1.02s/it]\n",
      "Epoch 18/20: 100%|██████████| 379/379 [06:19<00:00,  1.00s/it]\n",
      "Epoch 19/20: 100%|██████████| 379/379 [06:16<00:00,  1.01it/s]\n"
     ]
    }
   ],
   "source": [
    "output_folder = \"../output/\"\n",
    "# Load and modify the pretrained InceptionV1 model\n",
    "model = models.googlenet(weights=models.GoogLeNet_Weights.IMAGENET1K_V1, transform_input=False)\n",
    "num_classes = len(train_dataset.classes)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Learning rate scheduler\n",
    "def warmup_cosine_annealing(lr, warmup_epochs, total_epochs, warmup_start_lr=1e-5):\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            return (lr - warmup_start_lr) / warmup_epochs * epoch + warmup_start_lr / lr\n",
    "        elif total_epochs > warmup_epochs:\n",
    "            return 0.5 * (1 + torch.cos(torch.tensor(math.pi * (epoch - warmup_epochs) / (total_epochs - warmup_epochs))))\n",
    "        else:\n",
    "            return 1\n",
    "    return lr_lambda\n",
    "\n",
    "num_epochs = 20\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=warmup_cosine_annealing(0.001, 5, num_epochs))\n",
    "\n",
    "# TensorBoard setup\n",
    "writer = SummaryWriter('../runs/inceptionv1_experiment')\n",
    "\n",
    "# Initialize a DataFrame to store epoch, training loss, validation accuracy, and test accuracy\n",
    "df = pd.DataFrame(columns=['Epoch', 'Training Loss', 'Validation Accuracy', 'Test Accuracy', 'Epoch Duration'])\n",
    "df = df.astype({'Epoch': 'int32', 'Training Loss': 'float64', 'Validation Accuracy': 'float64', 'Test Accuracy': 'float64', 'Epoch Duration': 'float64'})\n",
    "\n",
    "# Initialize best validation accuracy\n",
    "best_val_accuracy = 0\n",
    "\n",
    "# Training loop with model checkpointing and TensorBoard logging\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=True)\n",
    "    for inputs, labels in progress_bar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    writer.add_scalar('Loss/train', avg_train_loss, epoch)\n",
    "    \n",
    "    # Test Phase\n",
    "    model.eval()\n",
    "    total_test = 0\n",
    "    correct_test = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "    test_accuracy = correct_test / total_test\n",
    "    writer.add_scalar('Accuracy/test', test_accuracy, epoch)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in validation_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_accuracy = correct / total\n",
    "    writer.add_scalar('Accuracy/val', val_accuracy, epoch)\n",
    "\n",
    "    epoch_duration = time.time() - start_time\n",
    "    \n",
    "    # Add data to DataFrame\n",
    "    new_row = pd.DataFrame({\n",
    "        'Epoch': [epoch + 1], \n",
    "        'Training Loss': [avg_train_loss], \n",
    "        'Validation Accuracy': [val_accuracy],\n",
    "        'Test Accuracy': [test_accuracy],\n",
    "        'Epoch Duration': [epoch_duration]\n",
    "    })\n",
    "    df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "    # Checkpointing\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), os.path.join(output_folder, f'inceptionv1_epoch{epoch}.pth'))\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "writer.close()\n",
    "\n",
    "# Save the DataFrame to a CSV file with a timestamp\n",
    "output_folder = \"../output/\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "csv_filename = f\"inceptionv1_training_results_{timestamp}.csv\"\n",
    "df.to_csv(os.path.join(output_folder, csv_filename), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Plotting the training loss and validation accuracy\u001b[39;00m\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m----> 5\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m'\u001b[39m], df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Loss\u001b[39m\u001b[38;5;124m'\u001b[39m], linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m'\u001b[39m], df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m], linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-.\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgreen\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m'\u001b[39m], df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m], linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-.\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the training loss and validation accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df['Epoch'], df['Training Loss'], linestyle='-', label='Training Loss')\n",
    "plt.plot(df['Epoch'], df['Test Accuracy'], linestyle='-.', color='green', label='Test Accuracy')\n",
    "plt.plot(df['Epoch'], df['Validation Accuracy'], linestyle='-.', color='red', label='Validation Accuracy')\n",
    "plt.title('Training Loss and Validation Accuracy per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Value')\n",
    "plt.xticks(range(1, num_epochs + 1, 2))  # Set x-axis ticks to be every 2 epochs\n",
    "plt.grid(True, which='both', axis='both', linestyle='--', color=\"#eaeaea\")\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(output_folder, f\"training_plot_{timestamp}.png\"), dpi=300)  # Save the plot as a PNG file\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_all_predictions(loader, model, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.view(-1).tolist())\n",
    "            all_labels.extend(labels.view(-1).tolist())\n",
    "    return all_preds, all_labels\n",
    "\n",
    "# Get predictions and true labels for both datasets\n",
    "train_preds, train_labels = get_all_predictions(train_loader, model, device)\n",
    "val_preds, val_labels = get_all_predictions(validation_loader, model, device)\n",
    "\n",
    "# Compute confusion matrices\n",
    "train_cm = confusion_matrix(train_labels, train_preds)\n",
    "val_cm = confusion_matrix(val_labels, val_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(30, 15), dpi=300)\n",
    "\n",
    "ConfusionMatrixDisplay(train_cm, display_labels=train_dataset.classes).plot(ax=axes[0], cmap=plt.cm.Blues)\n",
    "axes[0].set_title('Training Confusion Matrix')\n",
    "axes[0].set_xticklabels(rotation=45, labels=train_dataset.classes, ha= 'right')\n",
    "\n",
    "ConfusionMatrixDisplay(val_cm, display_labels=validation_dataset.classes).plot(ax=axes[1], cmap=plt.cm.Reds)\n",
    "axes[1].set_title('Validation Confusion Matrix')\n",
    "axes[1].set_xticklabels(rotation=45, labels=validation_dataset.classes, ha='right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Randomly select 10 images from the validation set\n",
    "random_indices = random.sample(range(len(validation_dataset)), 10)\n",
    "images, true_labels = zip(*[validation_dataset[i] for i in random_indices])\n",
    "\n",
    "# Function to unnormalize and convert a tensor to a PIL image\n",
    "def unnormalize_and_convert_to_pil(tensor):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(-1, 1, 1).to(tensor.device)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(-1, 1, 1).to(tensor.device)\n",
    "    tensor = tensor.clone().detach()  # Clone the tensor\n",
    "    tensor = tensor.mul_(std).add_(mean)  # Unnormalize\n",
    "    tensor = tensor.clamp(0, 1)  # Clamp to the range [0, 1]\n",
    "    return to_pil_image(tensor)\n",
    "\n",
    "# Predict and display\n",
    "plt.figure(figsize=(30, 20), dpi=300)\n",
    "for i, (image, true_label) in enumerate(zip(images, true_labels)):\n",
    "    # Predict\n",
    "    image_tensor = image.unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predicted_label = validation_dataset.classes[predicted[0]]\n",
    "\n",
    "    # Convert image for display\n",
    "    image_pil = unnormalize_and_convert_to_pil(image)\n",
    "\n",
    "    # Display\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(image_pil)\n",
    "    plt.title(f'Predicted: {predicted_label}\\nTrue: {validation_dataset.classes[true_label]}')\n",
    "    plt.axis('off')\n",
    "plt.show()\n",
    "plt.savefig('../output/sample_predictions_ResNet18.jpg', dpi=300, bbox_inches='tight', pad_inches=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

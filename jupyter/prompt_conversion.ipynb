{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file '../datasets/image_prompts.xlsx' created successfully.\n"
     ]
    }
   ],
   "source": [
    "import markdown\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Function to convert headings to scene types based on the provided structure\n",
    "def section_to_scene_type(section):\n",
    "    section = section.strip()\n",
    "    if section.startswith('1.1') or section.startswith('1.2'):\n",
    "        return 'nat'  # Natural scenes\n",
    "    elif section.startswith('1.3'):\n",
    "        return 'urban'  # Urban scenes\n",
    "    elif section.startswith('1.4'):\n",
    "        return 'flat'  # Flat scenes\n",
    "    return None\n",
    "\n",
    "# Convert markdown to Excel based on section types\n",
    "def convert_md_to_excel_with_outline(markdown_file, excel_file):\n",
    "    # Read the markdown file\n",
    "    with open(markdown_file, 'r') as file:\n",
    "        md_content = file.read()\n",
    "\n",
    "    # Define section mapping based on the outline structure\n",
    "    section_mapping = {\n",
    "        \"1.1\": \"Detailed Natural Scene descriptions\",\n",
    "        \"1.2\": \"Short Natural Scene descriptions\",\n",
    "        \"1.3\": \"Short Urban Scene descriptions\",\n",
    "        \"1.4\": \"Short Flat Scene descriptions\",\n",
    "    }\n",
    "\n",
    "    # Split the markdown content into sections using regex to capture the section numbers and descriptions\n",
    "    sections = re.split(r'(1\\.\\d\\s[^\\n]+)', md_content)\n",
    "\n",
    "    # Define the model-device mapping\n",
    "    model_device_mapping = {\n",
    "        'dalle2': 'G02',\n",
    "        'dalle3': 'G03',\n",
    "        'gpt3.5-turbo-0125': 'G04',\n",
    "        'gpt4-turbo': 'G05'\n",
    "    }\n",
    "\n",
    "    # Prepare data for the Excel file\n",
    "    rows = []\n",
    "    current_scene_type = None\n",
    "    prompt_counter = 1\n",
    "\n",
    "    # Process each section and classify prompts accordingly\n",
    "    for i in range(1, len(sections), 2):  # Step through the split sections\n",
    "        section_heading = sections[i].strip()  # Get the section heading (1.1, 1.2, 1.3, etc.)\n",
    "        current_scene_type = section_to_scene_type(section_heading)\n",
    "\n",
    "        if current_scene_type:\n",
    "            # Extract the prompts in the corresponding section\n",
    "            prompts = re.findall(r'\\d+\\.\\s+(.+)', sections[i + 1])  # Extract prompts\n",
    "\n",
    "            for prompt in prompts:\n",
    "                for model_name, device_code in model_device_mapping.items():\n",
    "                    # File name format: GXX_I_nat_XXXX\n",
    "                    file_name = f\"{device_code}_I_{current_scene_type}_{prompt_counter:04d}\"\n",
    "                    rows.append({\n",
    "                        'model': model_name,\n",
    "                        'file_name': file_name,\n",
    "                        'prompt': prompt,\n",
    "                        'nat/flat': current_scene_type\n",
    "                    })\n",
    "                prompt_counter += 1\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Write to Excel\n",
    "    df.to_excel(excel_file, index=False)\n",
    "    print(f\"Excel file '{excel_file}' created successfully.\")\n",
    "\n",
    "# Convert markdown to Excel using the provided outline structure\n",
    "markdown_file = '../datasets/image_descriptions.md'  # Path to your markdown file\n",
    "excel_file = '../datasets/image_prompts.xlsx'  # Output Excel file\n",
    "convert_md_to_excel_with_outline(markdown_file, excel_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts: 100%|██████████| 396/396 [00:00<00:00, 20842.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated prompts saved to ../datasets/image_prompts.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import tiktoken  # Tokenizer library for counting tokens\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set up your OpenAI API key from environment variable\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Load the OpenAI tokenizer model for GPT-4\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-4\")  # Adjust model as needed\n",
    "\n",
    "# Function to truncate prompt to 300 characters for DALL·E-2\n",
    "def truncate_to_300(prompt):\n",
    "    return prompt[:300]  # Truncate the prompt to the first 300 characters\n",
    "\n",
    "# Function to count tokens and truncate prompt if it exceeds the max token limit (3000 for DALL·E-3)\n",
    "def truncate_to_3000_tokens(prompt, max_tokens=3000):\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    if len(tokens) > max_tokens:\n",
    "        truncated_tokens = tokens[:max_tokens]\n",
    "        truncated_prompt = tokenizer.decode(truncated_tokens)\n",
    "        return truncated_prompt\n",
    "    return prompt\n",
    "\n",
    "# Function to use GPT-4 to compress the prompt while retaining meaning\n",
    "def compress_prompt_with_gpt4(prompt):\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Please reduce the following prompt to 3000 tokens while keeping its meaning: {prompt}\"}\n",
    "            ]\n",
    "        )\n",
    "        return response['choices'][0]['message']['content']\n",
    "    except Exception as e:\n",
    "        print(f\"Error using GPT-4 for prompt reduction: {e}\")\n",
    "        return prompt  # If there is an error, return the original prompt\n",
    "\n",
    "# Main function to process the Excel file, reduce prompt sizes, and save in new columns\n",
    "def update_excel_with_reduced_prompts(excel_file):\n",
    "    # Read the Excel file\n",
    "    df = pd.read_excel(excel_file)\n",
    "\n",
    "    # Create new columns for truncated prompts\n",
    "    prompt_300 = []\n",
    "    prompt_3k = []\n",
    "\n",
    "    # Iterate through each prompt in the DataFrame\n",
    "    for prompt in tqdm(df['prompt'], desc=\"Processing prompts\"):\n",
    "        # Truncate to 300 characters for DALL·E-2\n",
    "        prompt_300.append(truncate_to_300(prompt))\n",
    "        \n",
    "        # For DALL·E-3, compress or truncate prompt to 3000 tokens\n",
    "        if len(tokenizer.encode(prompt)) > 3000:\n",
    "            compressed_prompt = compress_prompt_with_gpt4(prompt)\n",
    "            prompt_3k.append(truncate_to_3000_tokens(compressed_prompt))\n",
    "        else:\n",
    "            prompt_3k.append(truncate_to_3000_tokens(prompt))\n",
    "\n",
    "    # Add new columns to the DataFrame\n",
    "    df['prompt_300'] = prompt_300\n",
    "    df['prompt_3k'] = prompt_3k\n",
    "\n",
    "    # Save the updated DataFrame back to the Excel file\n",
    "    df.to_excel(excel_file, index=False)\n",
    "    print(f\"Updated prompts saved to {excel_file}\")\n",
    "\n",
    "# Path to your Excel file\n",
    "excel_file = '../datasets/image_prompts.xlsx'\n",
    "\n",
    "# Update the Excel file with reduced prompts\n",
    "update_excel_with_reduced_prompts(excel_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
